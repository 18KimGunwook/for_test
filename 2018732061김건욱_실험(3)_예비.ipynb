{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2018732061김건욱_실험(3)_예비.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1N35fUrNdKmNF62HYtwooDdugjr1a6zVd","authorship_tag":"ABX9TyNm4phq7HXRbGQg+mErhKfo"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#실험 3 예비보고서\n","\n","***\n","###1. 실험제목\n","#### Linear Regression\n","###2.개요  \n","Linear Regression를 통해 함수를 모델링하는 방법과, gradient descent 사용해 Mean-Square-Erro를 cost를 하여 오차를 줄여 정확한 함수를 구할 수 있는 방법을 익한다.\n","###3.이론 \n","\n","###Linear Regression(선형 회귀)\n","\n","선형회귀 한 어떠한 변수가 특정 변수에 따라 변할 때 이 관계를 선형 상관 관계라 가정하는 것을 말한다. 이를 통해 변수들의 관계를 정령화 할 수 있으며, 값 또한 예측할 수 있게 된다.  \n","이를 markdown에서는 `torch.nn` 라이브러리의 `nn.Linear( , )` 함수를 통해 실행할 수 있다.\n","`nn.Linear` 함수는 3개의 input을 받으며 각각 받는 변수의 크기, 내보내는 변수의 크기, additive bias의 유무를 결정하는 변수이다. 마지막 input은 입력하지 않을 경우 true가 되도록 설정되어 있다.   \n","이를 통해 나오는 output은 다음식  \n","\n","$$\n","\\begin{aligned}\n","y_{hypo}=H(x)=w \\cdot x + b\n","\\end{aligned}\n","$$ \n","을 만족한다.\n","\n","###Mean Squre Error(최소 제곱법)  \n","Linear Regression를 통해 Hypothesis(가정)한 값은 train(관찰)값과 오차를 가지는 데 이를 제곱하여 나타난 값을 Mean Squre Error, MSE라 하며 이를 나타낸 식은 다음과 같다.\n","\n","$$\n","\\begin{aligned}\n","\t\\frac{1}{N}\\sum_{n=1}^{n=N}=(y_{hypo,(n)}-y_{train,(n)})^2\n","\\end{aligned}\n","$$ \n","\n","MSE 또한 markdown에서 제공하는 기능인 `mse_loss()`함수를 통해 계산할 수 있다.\n","\n","###Gradient Descent\n","위의 MSE가 최소가 될수록 오차가 더 작아진다는 의미이며 이는 더욱 정확한 식임을 의미하는 것이기 때문에 MSE의 최소값을 찾는 작업이 필요한데 이때 쓰이는 것이 Gradient Descent(경사 강하법)이다.  \n","경사강하법이란 gradient(기울기)가 양수일 때 음의 방향으로 x를 옮기고, 기울기가 음수일때 양의 방향으로 x를 옮기며 최소값을 찾는 방법이다.\n","이를 markdown에서는 `optimizer = opt.SGD(model.parameters(), lr= float)` 함수를 통해 x를 움직일 간격을 정하고 다음 과정을 통해  \n","`optimizer.zero_grad()  \n","cost.backward()   \n","optimizer.step()` \n","최소값의 방향으로 움직이게 된다. 이를 반복 할수록 최소값에 근사하게 된다. \n","\n","###4.실습\n","\n","[EX3_Linear_Regression](https://github.com/chorok-daddy/courses/blob/main/ML_Exp/EX3_Linear_Regression.ipynb) 의 코드를 직접 쳐보며 위의 내용을 연습한다.  \n","또한 행렬 방정식을 Linear_Regression 방식으로 풀고 'Pseudo Invers'를 이용한 풀이와 비교해본다.\n","\n","###5.참고문헌\n","\n","정승기 교수님의 [강의영상]( https://kwcommons.kw.ac.kr/em/6215cf4d6cb17),[강의노트](https://github.com/chorok-daddy/courses/tree/main/ML_Exp/)"],"metadata":{"id":"VPNeoNzqUh8H"}}]}